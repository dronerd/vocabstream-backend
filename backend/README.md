ğŸ“˜ VocabStream Backend (FastAPI)

This is the backend API for the VocabStream application.
It provides English conversation features powered by OpenAI, with adjustable language levels and specialties.
Built using FastAPI, designed for future integration with machine learning models in Python.

ğŸš€ Features

ğŸŒ REST API built with FastAPI

ğŸ§  AI chat powered by OpenAI API

âš™ï¸ Adjustable English level and specialty

ğŸ” Secure environment variable management

â˜ï¸ Deployable on Render with one file (render.yaml)

ğŸ”„ Supports auto-deployment from GitHub pushes

ğŸ§© Ready for ML model integration (PyTorch, Transformers, etc.)

ğŸ§± Project Structure
backend/
 â”œâ”€ main.py               # FastAPI entry point
 â”œâ”€ requirements.txt      # Python dependencies
 â”œâ”€ .env                  # Environment variables (not committed)
 â”œâ”€ .gitignore            # Git ignore rules
 â”œâ”€ render.yaml           # Render deployment blueprint
 â””â”€ README.md             # Documentation (this file)

âš™ï¸ Setup (Local Development)
1. Clone the repository
git clone https://github.com/<yourusername>/vocabstream-backend.git
cd vocabstream-backend

2. Create a virtual environment
python -m venv venv
source venv/bin/activate       # macOS/Linux
venv\Scripts\activate          # Windows

3. Install dependencies
pip install -r requirements.txt

4. Create a .env file

Create a file named .env in the project root:

OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxx


âš ï¸ Never commit your .env file to GitHub â€” itâ€™s private.

5. Run the FastAPI app
uvicorn main:app --reload


âœ… Open http://127.0.0.1:8000
 to confirm itâ€™s running.
âœ… You can test the chat endpoint using http://127.0.0.1:8000/docs
.

ğŸ§  Example API Call

Endpoint:

POST /api/chat


Body (JSON):

{
  "message": "Hello!",
  "level": "B2",
  "specialty": "Computer Science"
}


Response:

{
  "reply": "Hi there! How are you today?"
}

â˜ï¸ Deployment (Render)
1. Push to GitHub

Commit your code and push it:

git add .
git commit -m "Initial backend setup"
git push origin main

2. Create a Render Blueprint

Render uses render.yaml to auto-configure everything.
Hereâ€™s what it does:

services:
  - type: web
    name: vocabstream-backend
    env: python
    buildCommand: pip install -r requirements.txt
    startCommand: uvicorn main:app --host 0.0.0.0 --port $PORT
    autoDeploy: true
    envVars:
      - key: PYTHON_VERSION
        value: 3.11
      - key: OPENAI_API_KEY
        sync: false

3. Deploy

Go to Render.com

Click â€œNew +â€ â†’ â€œBlueprintâ€

Connect your GitHub repo

Render auto-detects render.yaml

Add your OPENAI_API_KEY in Environment Variables

Click Deploy Blueprint

https://vocabstream-backend.onrender.com

ğŸ”— Connecting to Frontend

In your React app, update your fetch URL:

const res = await fetch("https://vocabstream-backend.onrender.com/api/chat", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({
    message: userInput,
    level,
    specialty,
  }),
});

ğŸ§© Future Enhancements

Add user authentication (JWT / OAuth)

Integrate custom ML models (e.g. BERT, T5, Whisper)

Store chat history in a database (PostgreSQL, MongoDB)

Implement speech input/output endpoints

Add rate limiting for public API safety

